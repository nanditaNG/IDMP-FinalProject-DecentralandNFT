---
title: "NFT Decentraland Trends Predictions and Analysis"
authors: "Nandita Gurwara, Shweta Mishra, Shailesh Chikne, Rishabh Chhaparia"
date: "04/01/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
options(warn = -1)
library(rlang)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(modelr)
library(stringr)
library(ggpubr)
#install.packages("htmlwidgets")
```

```{r warning=FALSE}
# Read the data from CSV files

nftdf <- read_csv("./NFT_Decentraland.csv", col_types = cols())

#Adding columns createdAt, updatedAt and expiresAt to the dataset
nftdf <- nftdf %>%
  mutate(
    createdAt = as.POSIXct(createdAt, origin = "1970-01-01"),
    updatedAt = as.POSIXct(updatedAt, origin = "1970-01-01"),
    expiresAt = as.POSIXct(expiresAt * 0.001, origin = "1970-01-01"),
    price = price / (10**18)
  )


df_split <- split(nftdf, nftdf$nftAddress)

for (i in 1:length(df_split)) {
  df_split[[i]] <- df_split[[i]] %>%
    mutate(sale_type = ifelse(createdAt == min(df_split[[i]]$createdAt),
      "Primary", "Secondary"
    ))
}

nftdf_updated <- bind_rows(df_split) %>%
  arrange(desc(createdAt))
nftdf_updated
```

```{r}
# Install package to read image
#install.packages("imager")
#library(stringr)
regexp <- "'image': '[^']*"
nftdf_updated$image <- str_match_all(nftdf_updated$nft, regexp) %>%
  str_replace_all("'image': '", "")

write.csv(nftdf_updated, "./NFT_Decentraland_Transformed.csv")

# method to load images
# load.image(nftdf_updated$image[1])
```

### Add wearable category

```{r}
nftDf <-
  read_csv("./NFT_Decentraland_Transformed.csv", col_types = cols())
regexp <- "'searchWearableCategory': '[^']*"
nftDf$wearableCategory <-
  str_match_all(nftdf_updated$nft, regexp) %>%
  str_replace_all("'searchWearableCategory': '", "")
nftDf$wearableCategory[nftDf$wearableCategory == "character(0)"] = NA
write.csv(nftDf, "./NFT_Decentraland_Transformed.csv")

```

## Top 5 NFTs Category

```{r echo=FALSE}
#install.packages("psych")
library(psych)
nftdf_updated_tmp <- nftdf_updated %>%
mutate(category = case_when(category == "parcel" ~ 1,
category == "estate" ~ 2,
category == "ens" ~ 3,
category == "wearable" ~ 4 ))

ggplot(data = nftdf_updated_tmp, mapping = aes(x = category,
y = sale_type,
fill = log2(price))) +
geom_tile() +
labs(x = "Category", y = "Sale type", fill = "Price", title = 'Heatmap category v/s price')

corPlot(nftdf_updated_tmp[c('category', 'price')], cex = 1.2)
```

```{r}
#Top 5 NFTs per category as tables with images

#Install all the packages before running the code

library(gridExtra)
library(grid)
library(htmlwidgets)
library(webshot)
library(reactablefmtr)
library(tidyverse)
library(nflfastR)

#top5 with id 

top5 <- nftdf_updated %>% 
  select(category, price,image) %>%
  group_by(category) %>%
  arrange(desc(price)) %>%
  top_n(5,price)

#top 5 without id

top5_id <- nftdf_updated %>% 
  select(id,category, price,image) %>%
  group_by(category) %>%
  arrange(desc(price)) %>%
  top_n(5,price)

#filtering dataset by category for top 5

data_estate <- filter(top5,category=='estate')
data_parcel <- filter(top5,category=='parcel')
data_ens <- filter(top5,category=='ens')
data_wearable <-filter(top5,category=='wearable')

#filtering dataset by category for bottom 5

data_estate_bottom <- filter(bottom_5,category=='estate')
data_wearable_bottom <- filter(bottom_5,category=='wearable')
data_parcel_bottom <- filter(bottom_5,category=='parcel')
data_ens_bottom <- filter(bottom_5,category=='ens')
data_wear_bottom <- head(data_wearable_bottom)

#install.packages("reactablefmtr")
#install.packages("nflfastR")
#install.packages("webshot")
#install.packages("htmlwidgets")
#webshot::install_phantomjs()


#Displaying top 5 estate as a table with respective images

reactable(data_estate)

reactable(data_estate,
          columns = list(image = colDef(cell = embed_img())))

df1 <- reactable(data_estate,
          columns = list(image = colDef(cell = embed_img(
            height = "50", width = "50"
          ))))

html_file1 <- "Estate.html"
img_file1 <- "Estate.png"

#table1 <- reactable(df1)
saveWidget(widget = df1, file = html_file1, selfcontained = TRUE)
webshot(url = html_file1, file = img_file1, delay = 0.1, vwidth = 500,vheight = 500)

```
```{r}

#Displaying top 5 wearable as a table with respective images

reactable(data_wearable)

reactable(data_wearable,
          columns = list(image = colDef(cell = embed_img())))

df2 <- reactable(data_wearable,
          columns = list(image = colDef(cell = embed_img(
            height = "50", width = "50"
          ))))

html_file2 <- "wearable.html"
img_file2 <- "wearable.png"

#table <- reactable(df)
saveWidget(widget = df2, file = html_file2, selfcontained = TRUE)
webshot(url = html_file2, file = img_file2, delay = 0.1, vwidth = 500,vheight = 500)
```
```{r}
#Displaying top 5 parcel as a table with respective images

reactable(data_parcel)

reactable(data_parcel,
          columns = list(image = colDef(cell = embed_img())))

df3 <- reactable(data_parcel,
          columns = list(image = colDef(cell = embed_img(
            height = "50", width = "50"
          ))))

html_file3 <- "parcel.html"
img_file3 <- "parcel.png"

#table <- reactable(df)
saveWidget(widget = df3, file = html_file3, selfcontained = TRUE)
webshot(url = html_file3, file = img_file3, delay = 0.1, vwidth = 500,vheight = 500)
```


```{r}
#Printing lowest priced NFTs here for all 3 categories 

#For estate
reactable(data_estate_bottom)

reactable(data_estate_bottom,
          columns = list(image = colDef(cell = embed_img())))

df_bottom <- reactable(data_estate_bottom,
          columns = list(image = colDef(cell = embed_img(
            height = "50", width = "50"
          ))))

html_file_bottom <- "Estate_bottom.html"
img_file_bottom <- "Estate_bottom.png"

#table1 <- reactable(df1)
saveWidget(widget = df_bottom, file = html_file_bottom, selfcontained = TRUE)
webshot(url = html_file_bottom, file = img_file_bottom, delay = 0.1, vwidth = 500,vheight = 500)

#For wearables

reactable(data_wear_bottom)

reactable(data_wear_bottom,
          columns = list(image = colDef(cell = embed_img())))

df_bottom_wear <- reactable(data_wear_bottom,
          columns = list(image = colDef(cell = embed_img(
            height = "50", width = "50"
          ))))

html_file_wear <- "wearable_bottom_wear.html"
img_file_wear <- "wearable_bottom_wear.png"

#table1 <- reactable(df1)
saveWidget(widget = df_bottom_wear, file = html_file_wear, selfcontained = TRUE)
webshot(url = html_file_wear, file = img_file_wear, delay = 0.1, vwidth = 500,vheight = 500)

#For parcel

reactable(data_parcel_bottom)

reactable(data_parcel_bottom,
          columns = list(image = colDef(cell = embed_img())))

df_bottom_parcel <- reactable(data_parcel_bottom,
          columns = list(image = colDef(cell = embed_img(
            height = "50", width = "50"
          ))))

html_file_parcel <- "parcel_bottom.html"
img_file_parcel <- "parcel_bottom.png"

#table1 <- reactable(df1)
saveWidget(widget = df_bottom_parcel, file = html_file_parcel, selfcontained = TRUE)
webshot(url = html_file_parcel, file = img_file_parcel, delay = 0.1, vwidth = 500,vheight = 500)

```

```{r}
#png("Parcel.png",height=300,width=1000)
#grid.table(as.data.frame(data_parcel), theme =tt3)
#dev.off()
```

```{r}
#top 5 ens nft
#Displaying ens as table

png("Ens.png",height=300,width=1000)
grid.table(as.data.frame(data_ens), theme =tt3)
dev.off()
```

```{r}
#bottom 5 ens nft
#Displaying ens as table

png("Ens_bottom.png",height=300,width=1000)
grid.table(as.data.frame(data_ens_bottom), theme =tt3)
dev.off()
```

# EDA -- Price trend among the categories

## A

## NFT distribution among categories

```{r echo=FALSE}
library(ggplot2)
ggplot(nftDf,aes(x=category))+
  geom_histogram(stat="count",aes(fill=category),show.legend = FALSE)+
labs(title="Wearable has the highest amount of NFTs",
     x="Different Categories of NFT")+
theme_classic()
```

## B

# Price and SD across ctaegories

```{r echo=FALSE}
library(ggplot2)
ggplot(nftDf,aes(x=category))+
  geom_boxplot(aes(y=log10(price),
                     fill=category),
                 show.legend = FALSE)+
labs(title="Estate has the highest average price
but the spread is more in Wearable.",
     x="Categories of NFT",
     y="Price of NFTs")+
theme_classic()
```

## C

# PRICE TRENDS OF TOP 5 

# adding createdAt column for Visualization

```{r tello}
top5_id1 <- nftDf %>% 
  select(id,category, price,image,createdAt) %>%
  group_by(category) %>%
  arrange(desc(price)) %>%
  top_n(5,price)
```

#trying to visualize each category in different plots, hence segregating the data, also adding createdAt column

```{r test}
data_estate1 <- filter(top5_id1,category=='estate')
data_parcel1 <- filter(top5_id1,category=='parcel')
data_ens1 <- filter(top5_id1,category=='ens')
data_wearable1 <- filter(top5_id1,category=='wearable')
```

```{r echo=FALSE}
library(ggplot2)
fun_color_range <- colorRampPalette(c("yellow", "black","red")) 
my_colors <- fun_color_range(20)
ggplot(top5_id1,aes(x=createdAt))+
  geom_point(aes(y=price
             ,color=price),alpha=0.1,size=0.07)+
scale_colour_gradientn(colors = my_colors)+   
labs(title="Change of NFT Price (in Crypto) over the years",
     x="Years",
     y="Price (in Crypto Currency)")+
facet_grid(~category)+ 
geom_smooth(aes(x=createdAt,
                y=price),
            color="Red",
            size=0.5)+
theme(axis.text.x=element_text(angle = 90))
```

# ESTATE : top 5 price trend 

```{r echo=FALSE}
library(ggplot2)
fun_color_range <- colorRampPalette(c("yellow", "black","red")) 
my_colors <- fun_color_range(20)
ggplot(data_estate1,aes(x=createdAt))+
  geom_point(aes(y=price
             ,color=price),alpha=0.1,size=0.07)+
scale_colour_gradientn(colors = my_colors)+   
labs(title="Change of NFT Price (in Crypto) over the years",
     x="Years",
     y="Price (in Crypto Currency)")+
geom_smooth(aes(x=createdAt,
                y=price),
            color="Red",
            size=0.5)+
theme(axis.text.x=element_text(angle = 90))
```

# PARCEL : top 5 price trend 

```{r echo=FALSE}
library(ggplot2)
fun_color_range <- colorRampPalette(c("yellow", "black","red")) 
my_colors <- fun_color_range(20)
ggplot(data_parcel1,aes(x=createdAt))+
  geom_point(aes(y=price
             ,color=price),alpha=0.1,size=0.07)+
scale_colour_gradientn(colors = my_colors)+   
labs(title="Change of NFT Price (in Crypto) over the years",
     x="Years",
     y="Price (in Crypto Currency)")+
geom_smooth(aes(x=createdAt,
                y=price),
            color="Red",
            size=0.5)+
theme(axis.text.x=element_text(angle = 90))
```

# ENS : top 5 price trend 

```{r echo=FALSE}
library(ggplot2)
fun_color_range <- colorRampPalette(c("yellow", "black","red")) 
my_colors <- fun_color_range(20)
ggplot(data_ens1,aes(x=createdAt))+
  geom_point(aes(y=price
             ,color=price),alpha=0.1,size=0.07)+
scale_colour_gradientn(colors = my_colors)+   
labs(title="Change of NFT Price (in Crypto) over the years",
     x="Years",
     y="Price (in Crypto Currency)")+
geom_smooth(aes(x=createdAt,
                y=price),
            color="Red",
            size=0.5)+
theme(axis.text.x=element_text(angle = 90))
```

# WEARABLE : top 5 price trend 

```{r echo=FALSE}
library(ggplot2)
fun_color_range <- colorRampPalette(c("yellow", "black","red")) 
my_colors <- fun_color_range(20)
ggplot(data_wearable1,aes(x=createdAt))+
  geom_point(aes(y=price
             ,color=price),alpha=0.1,size=0.07)+
scale_colour_gradientn(colors = my_colors)+   
labs(title="Change of NFT Price (in Crypto) over the years",
     x="Years",
     y="Price (in Crypto Currency)")+
geom_smooth(aes(x=createdAt,
                y=price),
            color="Red",
            size=0.5)+
theme(axis.text.x=element_text(angle = 90))
```

# HYPOTHESIS TESTING

## filtering for 2020 & 2021

```{r Dataset}
nft_updated_filtered <- nftDf %>% 
filter(createdAt>"2019-12-31 21:26:40")
```

# Hypothesis 1:

## Ho: “There is no impact of category on price.” 
## Ha: “Category influences price.”
## Alpha:0.05

#fit All

```{r 256}
library(modelr)
fit_All <- lm(price~category,data=nft_updated_filtered)
summary(fit_All)
```

### Failing to reject null hypothesis for wearable while being able to reject the rest of the categories. That implies categories indeed affect the NFT's price.

# Looking further into wearable category.

```{r Dataset_Estate}
nft_updated_filtered_W <- nft_updated_filtered %>% 
filter(category=='wearable')
```

```{r }
fit_W <- lm(price~wearableCategory,data=nft_updated_filtered_W)
summary(fit_W)
```

### On further analysis we found that the wearable sub-category has a confounding effect on price. On adding this predictor to our model, we were able to reject the null hypothesis and conclude that category indeed influences price.

## Hypothesis 2

#H0: price remains the same over the years for NFTs.
#Ha: price decreases over the years.
# alpha: 0.05

```{r 256}
library(modelr)
fit_E <- lm(price~createdAt,data=nftDf)
summary(fit_E)
```

## p-value is considerably small than alpha value. Hence, we reject the Null Hypothesis.Thus,price of NFTs decreases over the years.

## Hypothesis 3

### filtering estate

```{r Dataset_Estate}
nft_updated_filtered_E <- nft_updated_filtered %>% 
filter(category=='estate')
```

### H0: Mean price of Estate NFTs is 81000
### Ha: Mean price of Estate NFTs decreases over the years.
### Alpha:0.05

```{r 256}
library(modelr)
fit_M_E <- lm(price~createdAt,
              data=nft_updated_filtered_E)
summary(fit_M_E)
```

### p-value is considerably small than alpha value. Hence, we reject the Null Hypothesis. 

## CNN MODEL

```{r}
library(caret)
library(ggplot2)
library(pls)
library(mboost)

#to start with, include category as the only predictor
null_model <- lm(price ~ category, data = nftDf)
# full model includes all the predictors (worst case scenario)
full_model <-
  lm(price ~ category + as.numeric(createdAt) + log2(blockNumber),
     data = nftDf)
step_model <- step(
  null_model,
  scope = list(lower = null_model, upper = full_model),
  direction = "forward"
)


timeSlices <-
  createTimeSlices(
    1:nrow(nftDf),
    initialWindow = 12000,
    horizon = 100,
    fixedWindow = TRUE
  )

str(timeSlices, max.level = 1)
trainSlices <- timeSlices[[1]]
testSlices <- timeSlices[[2]]

plsFitTime <-
  train(
    price ~ category + as.numeric(createdAt) + log2(blockNumber),
    data = nftDf[trainSlices[[1]], ],
    method = "lm",
    preProc = c("center", "scale")
  )


pred <- predict(plsFitTime, nftDf[testSlices[[1]], ])

true <- nftDf$price[testSlices[[1]]]

plot(true,
     col = "red",
     ylab = "true (red) , pred (blue)",
     ylim = range(c(pred, true)))
points(pred, col = "blue")

plsFitTime

```

# CNN Classification
```{r}
library(keras)
library(magick)
library(tidyverse)
library(imager)
library(caret)

nft_w <- nftDf %>% filter(category == "wearable")
df <- distinct(nft_w, nft_w$image, nft_w$wearableCategory)
i <- 1
for (d in 1:nrow(df)) {
  path <- df[d, ]$`nft_w$image`
  dir <- df[d, ]$`nft_w$wearableCategory`
  img <- image_read(path)
  image_write(img, paste0("./train_data/", dir, "/img_", i, ".png"), format =
                "png")
  i <- i + 1
}



get_dim <- function(x) {
  img <- load.image(x)
  df_img <- data.frame(height = height(img),
                       width = width(img),
                       filename = x)
  return(df_img)
}

```

```{r}



# Desired height and width of images
target_size <- c(128, 128)



# Batch size for training the model
batch_size <- 32



# Image Generator
train_data_gen <-
  image_data_generator(
    rescale = 1 / 255,
    # Scaling pixel value
    horizontal_flip = T,
    # Flip image horizontally
    vertical_flip = T,
    # Flip image vertically
    rotation_range = 45,
    # Rotate image from 0 to 45 degrees
    zoom_range = 0.25,
    # Zoom in or zoom out range
    validation_split = 0.2 # 20% data as validation data
  )



# Training Dataset
train_image_array_gen <-
  flow_images_from_directory(
    directory = "./train_data/",
    # Folder of the data
    target_size = target_size,
    # target of the image dimension (64 x 64)
    color_mode = "rgb",
    # use RGB color
    batch_size = batch_size ,
    seed = 123,
    # set random seed
    subset = "training",
    # declare that this is for training data
    generator = train_data_gen
  )



# Validation Dataset
val_image_array_gen <-
  flow_images_from_directory(
    directory = "./train_data/",
    target_size = target_size,
    color_mode = "rgb",
    batch_size = batch_size ,
    seed = 123,
    subset = "validation",
    # declare that this is the validation data
    generator = train_data_gen
  )



# Number of training samples
train_samples <- train_image_array_gen$n



# Number of validation samples
valid_samples <- val_image_array_gen$n



# Number of target classes/categories
output_n <- n_distinct(train_image_array_gen$classes)



# Get the class proportion
table("\nFrequency" = factor(train_image_array_gen$classes)) %>%
  prop.table()
```


```{r}
# Set Initial Random Weight
tensorflow::tf$random$set_seed(123)



model <- keras_model_sequential(name = "simple_model") %>%
  
  # Convolution Layer
  layer_conv_2d(
    filters = 16,
    kernel_size = c(3, 3),
    padding = "same",
    activation = "relu",
    input_shape = c(target_size, 3)
  ) %>%
  
  
  
  # Max Pooling Layer
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Flattening Layer
  layer_flatten() %>%
  
  # Dense Layer
  layer_dense(units = 16,
              activation = "relu") %>%
  
  # Output Layer
  layer_dense(units = output_n,
              activation = "softmax",
              name = "Output")

model
```

```{r}
model %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_adam(lr = 0.001),
          metrics = "accuracy")



# Fit data into model
history <- model %>%
  fit(
    # training data
    train_image_array_gen,
    
    
    
    # training epochs
    steps_per_epoch = as.integer(train_samples / batch_size),
    epochs = 30,
    
    # validation data
    validation_data = val_image_array_gen,
    validation_steps = as.integer(valid_samples / batch_size)
  )



plot(history)
```


```{r}

val_data <- data.frame(file_name = paste0("./train_data/",
                                          val_image_array_gen$filenames)) %>%
  mutate(
    class = str_extract(
      file_name,
      "earring|eyes|eyewear|feet|hair|hat|helmet|lower_body|mask|mouth|tiara|top_head|upper_body"
    )
  )

head(val_data, 10)
```


```{r}
# Function to convert image to array
image_prep <- function(x) {
          arrays <- lapply(x, function(path) {
                    img <- image_load(path, target_size = target_size,
                    grayscale = F # Set FALSE if image is RGB
                    )
                    
                    x <- image_to_array(img)
                    x <- array_reshape(x, c(1, dim(x)))
                    x <- x/255 # rescale image pixel
                    })
          do.call(abind::abind, c(arrays, list(along = 1)))
}
```


```{r}
test_x <- image_prep(val_data$file_name)



# Check dimension of testing data set
dim(test_x)
```


```{r}
library(matrixStats)
pred_test <- predict(model, test_x)

df1 <- pred_test

pred_test <- apply(df1, 1, function(x)
  which(x == max(x)))

pred_test <- pred_test - 1

head(pred_test, 10)
```

```{r}
# Convert encoding to label
decode <- function(x) {
  case_when(
    x == 0 ~ "earring",
    x == 1 ~ "eyes",
    x == 2 ~ "eyewear",
    x == 3 ~ "feet",
    x == 4 ~ "hair",
    x == 5 ~ "hat",
    x == 6 ~ "helmet",
    x == 7 ~ "lower_body",
    x == 8 ~ "mask",
    x == 9 ~ "mouth",
    x == 10 ~ "tiara",
    x == 11 ~ "top_head",
    x == 12 ~ "upper_body"
  )
}



pred_test <- sapply(pred_test, decode)



head(pred_test, 10)
```

```{r}
confusionMatrix(as.factor(pred_test),
                as.factor(val_data$class))
```

##Big Model

``` {r}

tensorflow::tf$random$set_seed(123)

model_big <- keras_model_sequential() %>%
  
  # First convolutional layer
  layer_conv_2d(
    filters = 32,
    kernel_size = c(5, 5),
    # 5 x 5 filters
    padding = "same",
    activation = "relu",
    input_shape = c(target_size, 3)
  ) %>%
  
  # Second convolutional layer
  layer_conv_2d(
    filters = 32,
    kernel_size = c(3, 3),
    # 3 x 3 filters
    padding = "same",
    activation = "relu"
  ) %>%
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Third convolutional layer
  layer_conv_2d(
    filters = 64,
    kernel_size = c(3, 3),
    padding = "same",
    activation = "relu"
  ) %>%
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Fourth convolutional layer
  layer_conv_2d(
    filters = 128,
    kernel_size = c(3, 3),
    padding = "same",
    activation = "relu"
  ) %>%
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Fifth convolutional layer
  layer_conv_2d(
    filters = 256,
    kernel_size = c(3, 3),
    padding = "same",
    activation = "relu"
  ) %>%
  
  # Max pooling layer
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  
  # Flattening layer
  layer_flatten() %>%
  
  # Dense layer
  layer_dense(units = 64,
              activation = "relu") %>%
  
  # Output layer
  layer_dense(name = "Output",
              units = 13,
              activation = "softmax")

model_big


```


``` {r}

model_big %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_adam(lr = 0.001),
          metrics = "accuracy")

history <- model %>%
  fit_generator(
    # training data
    train_image_array_gen,
    
    # epochs
    steps_per_epoch = as.integer(train_samples / batch_size),
    epochs = 50,
    
    # validation data
    validation_data = val_image_array_gen,
    validation_steps = as.integer(valid_samples / batch_size),
    
    # print progress but don't create graphic
    verbose = 1,
    view_metrics = 0
  )

plot(history)
```


```{r}

pred_test <- predict(model_big, test_x)

pred_test <- apply(pred_test, 1, function(x)
  which(x == max(x)))

pred_test <- pred_test - 1

head(pred_test, 10)

pred_test <- sapply(pred_test, decode)

head(pred_test, 10)

```


```{r}
confusionMatrix(as.factor(pred_test),
                as.factor(val_data$class))

```

